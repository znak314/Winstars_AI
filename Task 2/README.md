# Task 2. NER + Image Classification

## Overview of the solution
We can divide this task into 2 separate tasks: NER problem and Image classification.

I worked on NER problem for animal detection in text. I created a list with 58 different animals names. For each of the names a sentence with this name was generated by using chatGPT. The details are in `EDA.ipynb`.
To build the model, I used the pre-trained [BERT](https://huggingface.co/docs/transformers/model_doc/bert) model and trained in on the created dataset.

For the image classification part I worked with [Animals-10](https://www.kaggle.com/datasets/alessiocorrado99/animals10) dataset from Kaggle. It contains about 28K medium quality animal images belonging to 10 categories: dog, cat, horse, spyder, butterfly, chicken, sheep, cow, squirrel, elephant. 
For the modelling part I tried `EfficientNet`, but since I was very limited in computing resources and time, I decided to use `ResNet`. 
**Important:** I only did the training on 3 epochs, due to lack of time, but how to improve it is described in the file `Boost_ideas.pdf`


Source code is written in Python 3.11. All code is written in OOP style with [SOLID](https://www.digitalocean.com/community/conceptual-articles/s-o-l-i-d-the-first-five-principles-of-object-oriented-design) principles.

## Project structure
- `Image_Classifier` - folder with Image Classifier files.
    -  `animal_classifier.py` - a file with model class.
    -  `data_loader.py` - a file with pre-processing `ImageDataPipeline` class.
    -  `inference.py` - classifier inference file.
    -  `train.py` - classifier training file.
- `NER` - folder with NER files.
    -  `ner_model_trainer.py` - NER training class file.
    -  `ner_model_predictor.py` - NER inference class file.
    -  `data_loader.py` -  a file with class for NER data loading.
    -  `inference.py` - NER training file.
    -  `train.py` - NER inference file.
- `data` - folder with data.
    -  `NER_data` - folder with data for NER files.
    -  `example_images` - folder with example images.
- `EDA.ipynb` - notebook with EDA details and NER dataset creation details.
- `demo_notebook.ipynb` - notebook with demonstration of the work.
- `pipeline.py` - a file with `Pipeline` class.
- `requirements.txt` - a file with required libs.

## Models
### NER
For NER task I used BERT - Bidirectional Encoder Representations from Transformers. It is used to detect and classify entities in the text, such as names of people, places, organizations, dates, numerical values, etc. For training and evaluation, the Simple Transformers library is used, which is a high-level wrapper for Hugging Face Transformers that simplifies the use of BERT model

### Image Classification
For Image classification I used custom model.
This model is a deep learning-based animal classifier built using a `ResNet-inspired` architecture in TensorFlow and Keras. 
It processes input images of size `224x224x3` and classifies them into one of ten categories. 
The model starts with a convolutional layer followed by batch normalization, `ReLU` activation, and max pooling to extract initial features. 
It includes several residual blocks that help preserve information by using skip connections, making the network more efficient in training deep architectures. 
The final layers apply global average pooling and a fully connected softmax layer to produce classification probabilities.
The model is optimized using the Adam optimizer with categorical cross-entropy loss and tracks accuracy during training. 
It supports training with validation data, evaluating performance on test data, and saving the trained model for future use.

## How to use?
### 1. **Clone the repository**
   Clone this repository to your local machine using:

   ```bash
git clone https://github.com/znak314/Winstars_AI.git
   ```
### 2. **Create a virtual environment**

   ```bash
cd "Task 2"
python -m venv .venv
.venv\Scripts\activate     # Windows 
source .venv/bin/activate  # Linux
   ```
### 3. **Install all necessary libraries**
   Install all dependencies by using:

   ```bash
   pip install -r requirements.txt
   ```
### 4. Install datasets
Install 'data' folder from [google drive](https://drive.google.com/drive/folders/1xkKDfIRDH2l8-c6OFX0boT-v825l5R2n)

### 5. Install models (optional)
You can also download my trained models from 'models' folder in [google drive](https://drive.google.com/drive/folders/1H8FNRVqBQQoMK5CPtP8TlyFF74hs7Xij?usp=drive_link)

### 6. Commands
To run NER training:
```bash
python NER/train.py --file_path "path_to_dataset" --epochs 3 --learning_rate 2e-4 --batch_size 16 --output_dir "output_dir_path"
```

To run Image Classifier training:
```bash
python Image_Classifier/train.py --dataset_path "dataset_path" --epochs 30 --batch_size 32
```

To run NER inference:

```bash
python NER/inference.py --model_path "models/trained_ner_model" --input_text "some_text"
```

To run Image Classifier inference:

```bash
python Image_Classifier/inference.py --img_path "path_to_some_img" --model_path "path_to_classifier_model"
```